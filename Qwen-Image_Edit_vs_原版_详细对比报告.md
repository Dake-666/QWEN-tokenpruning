# Qwen-Image-Edit ä¸åŸç‰ˆ Qwen-Image è¯¦ç»†å¯¹æ¯”æŠ¥å‘Š

## ğŸ“‹ æ‰§è¡Œæ‘˜è¦

Qwen-Image-Edit æ˜¯åœ¨ Qwen-Image åŸºç¡€ä¸Šä¸“é—¨ä¸ºå›¾åƒç¼–è¾‘ä»»åŠ¡è®¾è®¡çš„å˜ä½“ã€‚æ ¸å¿ƒåˆ›æ–°åœ¨äº**åŒè·¯å¾„è¾“å…¥æ¶æ„**ï¼šå°†è¾“å…¥å›¾åƒåŒæ—¶é€å…¥ Qwen2.5-VLï¼ˆè§†è§‰-è¯­è¨€æ¨¡å‹ï¼‰å’Œ VAE ç¼–ç å™¨ï¼Œå®ç°è¯­ä¹‰ç¼–è¾‘ä¸è§†è§‰å¤–è§‚æ§åˆ¶çš„æœ‰æœºç»“åˆã€‚

---

## ğŸ” ä¸€ã€æ¶æ„å±‚é¢å·®å¼‚

### 1.1 æ¨¡å‹ç»„ä»¶å¯¹æ¯”

| ç»„ä»¶ | Qwen-Image (åŸç‰ˆ) | Qwen-Image-Edit |
|------|------------------|-----------------|
| **Text Encoder** | Qwen2_5_VLForConditionalGeneration | âœ… ç›¸åŒ |
| **Tokenizer** | Qwen2Tokenizer | âœ… ç›¸åŒ |
| **Processor** | âŒ æ—  | âœ… **Qwen2VLProcessor** (æ–°å¢) |
| **Transformer** | QwenImageTransformer2DModel | âœ… ç›¸åŒ |
| **VAE** | AutoencoderKLQwenImage | âœ… ç›¸åŒ |
| **Scheduler** | FlowMatchEulerDiscreteScheduler | âœ… ç›¸åŒ |

**å…³é”®å‘ç°**ï¼šEdit ç‰ˆæœ¬æ–°å¢äº† `processor` ç»„ä»¶ï¼Œç”¨äºå¤„ç†å¤šæ¨¡æ€è¾“å…¥ï¼ˆæ–‡æœ¬+å›¾åƒï¼‰ã€‚

---

### 1.2 åˆå§‹åŒ–å‚æ•°å·®å¼‚

#### åŸç‰ˆ Qwen-Image
```python
def __init__(
    self,
    scheduler: FlowMatchEulerDiscreteScheduler,
    vae: AutoencoderKLQwenImage,
    text_encoder: Qwen2_5_VLForConditionalGeneration,
    tokenizer: Qwen2Tokenizer,
    transformer: QwenImageTransformer2DModel,
):
```

#### Edit ç‰ˆæœ¬
```python
def __init__(
    self,
    scheduler: FlowMatchEulerDiscreteScheduler,
    vae: AutoencoderKLQwenImage,
    text_encoder: Qwen2_5_VLForConditionalGeneration,
    tokenizer: Qwen2Tokenizer,
    processor: Qwen2VLProcessor,  # â­ æ–°å¢
    transformer: QwenImageTransformer2DModel,
):
```

**ä»£ç ä½ç½®**ï¼š
- åŸç‰ˆï¼š```154:160:pipelines/qwenimage/pipeline_qwenimage.py```
- Editç‰ˆï¼š```187:195:pipelines/qwenimage/pipeline_qwenimage_edit.py```

---

## ğŸ“ äºŒã€Prompt æ¨¡æ¿ä¸æ–‡æœ¬ç¼–ç å·®å¼‚

### 2.1 Prompt æ¨¡æ¿å¯¹æ¯”

#### åŸç‰ˆ Qwen-Image æ¨¡æ¿
```python
prompt_template_encode = (
    "<|im_start|>system\n"
    "Describe the image by detailing the color, shape, size, texture, quantity, "
    "text, spatial relationships of the objects and background:<|im_end|>\n"
    "<|im_start|>user\n{}<|im_end|>\n"
    "<|im_start|>assistant\n"
)
prompt_template_encode_start_idx = 34  # ä¸¢å¼ƒå‰34ä¸ªtoken
```

**åŠŸèƒ½**ï¼šå¼•å¯¼æ¨¡å‹**æè¿°**å›¾åƒå†…å®¹ï¼Œç”¨äºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ã€‚

#### Edit ç‰ˆæœ¬æ¨¡æ¿
```python
prompt_template_encode = (
    "<|im_start|>system\n"
    "Describe the key features of the input image (color, shape, size, texture, objects, background), "
    "then explain how the user's text instruction should alter or modify the image. "
    "Generate a new image that meets the user's requirements while maintaining consistency "
    "with the original input where appropriate.<|im_end|>\n"
    "<|im_start|>user\n"
    "<|vision_start|><|image_pad|><|vision_end|>{}<|im_end|>\n"
    "<|im_start|>assistant\n"
)
prompt_template_encode_start_idx = 64  # ä¸¢å¼ƒå‰64ä¸ªtokenï¼ˆç³»ç»Ÿæç¤ºæ›´é•¿ï¼‰
```

**åŠŸèƒ½**ï¼š
1. é¦–å…ˆç†è§£è¾“å…¥å›¾åƒçš„**å…³é”®ç‰¹å¾**
2. ç„¶åè§£é‡Šç”¨æˆ·çš„**ç¼–è¾‘æŒ‡ä»¤**å¦‚ä½•ä¿®æ”¹å›¾åƒ
3. ç”Ÿæˆç¬¦åˆè¦æ±‚çš„æ–°å›¾åƒï¼ŒåŒæ—¶ä¿æŒä¸åŸå›¾çš„ä¸€è‡´æ€§

**ä»£ç ä½ç½®**ï¼š
- åŸç‰ˆï¼š```176:177:pipelines/qwenimage/pipeline_qwenimage.py```
- Editç‰ˆï¼š```213:214:pipelines/qwenimage/pipeline_qwenimage_edit.py```

---

### 2.2 æ–‡æœ¬ç¼–ç æ–¹æ³•å·®å¼‚

#### åŸç‰ˆï¼šçº¯æ–‡æœ¬ç¼–ç ï¼ˆ`_get_qwen_prompt_embeds`ï¼‰

```python
def _get_qwen_prompt_embeds(
    self,
    prompt: Union[str, List[str]] = None,
    device: Optional[torch.device] = None,
    dtype: Optional[torch.dtype] = None,
):
    # 1. åªå¤„ç†æ–‡æœ¬
    txt = [template.format(e) for e in prompt]
    
    # 2. ä½¿ç”¨ tokenizer å¤„ç†æ–‡æœ¬
    txt_tokens = self.tokenizer(
        txt, max_length=self.tokenizer_max_length + drop_idx,
        padding=True, truncation=True, return_tensors="pt"
    ).to(device)
    
    # 3. ä»…ä½¿ç”¨æ–‡æœ¬è¾“å…¥è°ƒç”¨ text_encoder
    encoder_hidden_states = self.text_encoder(
        input_ids=txt_tokens.input_ids,
        attention_mask=txt_tokens.attention_mask,
        output_hidden_states=True,
    )
```

**ä»£ç ä½ç½®**ï¼š```188:224:pipelines/qwenimage/pipeline_qwenimage.py```

#### Edit ç‰ˆæœ¬ï¼šå¤šæ¨¡æ€ç¼–ç ï¼ˆåŒè·¯å¾„æ ¸å¿ƒå®ç°ï¼‰

```python
def _get_qwen_prompt_embeds(
    self,
    prompt: Union[str, List[str]] = None,
    image: Optional[torch.Tensor] = None,  # â­ æ–°å¢å›¾åƒè¾“å…¥
    device: Optional[torch.device] = None,
    dtype: Optional[torch.dtype] = None,
):
    # 1. å‡†å¤‡æ–‡æœ¬æ¨¡æ¿ï¼ˆåŒ…å«å›¾åƒå ä½ç¬¦ï¼‰
    txt = [template.format(e) for e in prompt]
    
    # 2. â­ ä½¿ç”¨ processor å¤„ç†å¤šæ¨¡æ€è¾“å…¥ï¼ˆæ–‡æœ¬+å›¾åƒï¼‰
    model_inputs = self.processor(
        text=txt,
        images=image,  # å›¾åƒåŒæ—¶è¾“å…¥
        padding=True,
        return_tensors="pt",
    ).to(device)
    
    # 3. â­ è°ƒç”¨ text_encoder åŒæ—¶å¤„ç†æ–‡æœ¬å’Œå›¾åƒ
    outputs = self.text_encoder(
        input_ids=model_inputs.input_ids,
        attention_mask=model_inputs.attention_mask,
        pixel_values=model_inputs.pixel_values,      # â­ å›¾åƒåƒç´ å€¼
        image_grid_thw=model_inputs.image_grid_thw,  # â­ å›¾åƒç½‘æ ¼ä¿¡æ¯
        output_hidden_states=True,
    )
```

**ä»£ç ä½ç½®**ï¼š```226:271:pipelines/qwenimage/pipeline_qwenimage_edit.py```

**å…³é”®å·®å¼‚**ï¼š
1. âœ… Edit ç‰ˆæœ¬æ¥å— `image` å‚æ•°
2. âœ… ä½¿ç”¨ `processor` åŒæ—¶å¤„ç†æ–‡æœ¬å’Œå›¾åƒ
3. âœ… `text_encoder` æ¥æ”¶ `pixel_values` å’Œ `image_grid_thw`ï¼ˆå›¾åƒç‰¹å¾ï¼‰

---

## ğŸ–¼ï¸ ä¸‰ã€åŒè·¯å¾„å›¾åƒå¤„ç†æ¶æ„

### 3.1 å›¾åƒè¾“å…¥æµç¨‹å¯¹æ¯”

#### åŸç‰ˆ Qwen-Imageï¼šæ— å›¾åƒè¾“å…¥
```
ç”¨æˆ·è¾“å…¥
  â””â”€> çº¯æ–‡æœ¬ Prompt
      â””â”€> Tokenizer
          â””â”€> Text Encoder (ä»…æ–‡æœ¬)
              â””â”€> Prompt Embeddings
```

#### Edit ç‰ˆæœ¬ï¼šåŒè·¯å¾„å¤„ç†

**è·¯å¾„1ï¼šè¯­ä¹‰ç†è§£è·¯å¾„ï¼ˆQwen2.5-VLï¼‰**
```
è¾“å…¥å›¾åƒ
  â””â”€> Qwen2VLProcessor (å¤šæ¨¡æ€å¤„ç†)
      â””â”€> Text Encoder (Qwen2.5-VL)
          â”œâ”€ pixel_values (å›¾åƒç‰¹å¾)
          â”œâ”€ image_grid_thw (å›¾åƒå¸ƒå±€)
          â””â”€ input_ids (æ–‡æœ¬token)
          â””â”€> å¤šæ¨¡æ€ Embeddings (è¯­ä¹‰ç†è§£)
```

**è·¯å¾„2ï¼šè§†è§‰å¤–è§‚è·¯å¾„ï¼ˆVAEï¼‰**
```
è¾“å…¥å›¾åƒ
  â””â”€> VaeImageProcessor (å›¾åƒé¢„å¤„ç†)
      â””â”€> VAE Encoder
          â””â”€> Image Latents (è§†è§‰ç‰¹å¾)
```

**ä»£ç å®ç°**ï¼š

1. **è·¯å¾„1 - Qwen2.5-VLï¼ˆè¯­ä¹‰æ§åˆ¶ï¼‰**
   ```python
   # encode_prompt ä¸­è°ƒç”¨
   prompt_embeds, prompt_embeds_mask = self.encode_prompt(
       image=prompt_image,  # â­ å›¾åƒé€å…¥VLæ¨¡å‹
       prompt=prompt,
       ...
   )
   ```
   ä½ç½®ï¼š```718:727:pipelines/qwenimage/pipeline_qwenimage_edit.py```

2. **è·¯å¾„2 - VAE Encoderï¼ˆå¤–è§‚æ§åˆ¶ï¼‰**
   ```python
   # prepare_latents ä¸­è°ƒç”¨
   image_latents = self._encode_vae_image(image=image, generator=generator)
   ```
   ä½ç½®ï¼š```395:416:pipelines/qwenimage/pipeline_qwenimage_edit.py```

---

### 3.2 VAE ç¼–ç æ–¹æ³•ï¼ˆEdit ç‰ˆæœ¬ç‰¹æœ‰ï¼‰

Edit ç‰ˆæœ¬æ–°å¢ `_encode_vae_image` æ–¹æ³•ï¼š

```python
def _encode_vae_image(self, image: torch.Tensor, generator: torch.Generator):
    # 1. VAE ç¼–ç å›¾åƒ
    image_latents = retrieve_latents(
        self.vae.encode(image), 
        generator=generator, 
        sample_mode="argmax"  # ä½¿ç”¨ argmax è€Œéé‡‡æ ·
    )
    
    # 2. å½’ä¸€åŒ–å¤„ç†ï¼ˆä¸ VAE é…ç½®ä¸€è‡´ï¼‰
    latents_mean = torch.tensor(self.vae.config.latents_mean)
    latents_std = torch.tensor(self.vae.config.latents_std)
    image_latents = (image_latents - latents_mean) / latents_std
    
    return image_latents
```

**ç‰¹ç‚¹**ï¼š
- ä½¿ç”¨ `sample_mode="argmax"` è€Œééšæœºé‡‡æ ·ï¼Œç¡®ä¿ç¼–ç ç¨³å®šæ€§
- è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†ï¼ŒåŒ¹é… VAE çš„æ½œåœ¨ç©ºé—´åˆ†å¸ƒ

**ä»£ç ä½ç½®**ï¼š```395:416:pipelines/qwenimage/pipeline_qwenimage_edit.py```

---

## ğŸ”„ å››ã€å»å™ªå¾ªç¯ä¸­çš„å·®å¼‚

### 4.1 Latent å‡†å¤‡é˜¶æ®µ

#### åŸç‰ˆ Qwen-Image
```python
def prepare_latents(
    self,
    image=None,  # æ— å›¾åƒè¾“å…¥
    batch_size,
    num_channels_latents,
    height, width,
    dtype, device, generator,
    latents=None,
):
    # ä»…ç”Ÿæˆéšæœºå™ªå£°
    if latents is None:
        latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)
        latents = self._pack_latents(latents, ...)
    
    return latents, None  # â­ æ—  image_latents
```

#### Edit ç‰ˆæœ¬
```python
def prepare_latents(
    self,
    image,  # â­ æ¥æ”¶å›¾åƒè¾“å…¥
    batch_size,
    num_channels_latents,
    height, width,
    dtype, device, generator,
    latents=None,
):
    # 1. å¦‚æœæä¾›äº†å›¾åƒï¼Œç¼–ç ä¸º latents
    if image is not None:
        image_latents = self._encode_vae_image(image=image, generator=generator)
        image_latents = self._pack_latents(image_latents, ...)
    
    # 2. å‡†å¤‡å»å™ªçš„åˆå§‹ latents
    if latents is None:
        latents = randn_tensor(shape, generator=generator, ...)
        latents = self._pack_latents(latents, ...)
    
    return latents, image_latents  # â­ è¿”å›ä¸¤è€…
```

**ä»£ç ä½ç½®**ï¼š
- åŸç‰ˆï¼š```399:420:pipelines/qwenimage/pipeline_qwenimage.py```
- Editç‰ˆï¼š```471:524:pipelines/qwenimage/pipeline_qwenimage_edit.py```

---

### 4.2 å»å™ªå¾ªç¯ä¸­çš„ Latent è¿æ¥

#### åŸç‰ˆ Qwen-Image
```python
for i, t in enumerate(timesteps):
    # ç›´æ¥ä½¿ç”¨ latents ä½œä¸ºè¾“å…¥
    latent_model_input = latents
    
    # è°ƒç”¨ Transformer
    noise_pred = self.transformer(
        hidden_states=latent_model_input,
        encoder_hidden_states=prompt_embeds,
        ...
    )
```

#### Edit ç‰ˆæœ¬
```python
for i, t in enumerate(timesteps):
    # â­ è¿æ¥å½“å‰å»å™ª latents ä¸åŸå§‹å›¾åƒ latents
    latent_model_input = latents
    if image_latents is not None:
        latent_model_input = torch.cat([latents, image_latents], dim=1)
        #             â†‘ å½“å‰å»å™ªçŠ¶æ€    â†‘ åŸå§‹å›¾åƒç‰¹å¾
    
    # è°ƒç”¨ Transformerï¼ˆæ¥æ”¶æ‹¼æ¥åçš„è¾“å…¥ï¼‰
    noise_pred = self.transformer(
        hidden_states=latent_model_input,  # â­ åŒ…å«åŸå§‹å›¾åƒä¿¡æ¯
        encoder_hidden_states=prompt_embeds,  # â­ åŒ…å«å¤šæ¨¡æ€è¯­ä¹‰
        ...
    )
    
    # åªå–å‰éƒ¨åˆ†ï¼ˆlatentså¯¹åº”éƒ¨åˆ†ï¼‰ä½œä¸ºé¢„æµ‹
    noise_pred = noise_pred[:, : latents.size(1)]
```

**ä»£ç ä½ç½®**ï¼š```810:828:pipelines/qwenimage/pipeline_qwenimage_edit.py```

**æŠ€æœ¯è¦ç‚¹**ï¼š
1. **Concatenation ç­–ç•¥**ï¼š`[å½“å‰å»å™ªlatents | åŸå§‹å›¾åƒlatents]`
2. **åŒå‘ä¿¡æ¯æµ**ï¼š
   - åŸå§‹å›¾åƒ latents â†’ æä¾›è§†è§‰å¤–è§‚å‚è€ƒ
   - æ–‡æœ¬+å›¾åƒ embeddings â†’ æä¾›è¯­ä¹‰ç¼–è¾‘æŒ‡ä»¤
3. **è¾“å‡ºæˆªå–**ï¼šTransformer è¾“å‡ºåªå–å‰ `latents.size(1)` éƒ¨åˆ†ï¼Œå¯¹åº”å»å™ªéƒ¨åˆ†

---

## ğŸ¯ äº”ã€åŠŸèƒ½å±‚é¢å·®å¼‚æ€»ç»“

### 5.1 è¾“å…¥å¤„ç†å·®å¼‚

| ç‰¹æ€§ | åŸç‰ˆ Qwen-Image | Edit ç‰ˆæœ¬ |
|------|----------------|-----------|
| **æ–‡æœ¬è¾“å…¥** | âœ… å¿…éœ€ | âœ… å¿…éœ€ |
| **å›¾åƒè¾“å…¥** | âŒ ä¸æ”¯æŒ | âœ… **å¿…éœ€**ï¼ˆç¼–è¾‘ç›®æ ‡å›¾åƒï¼‰ |
| **Processor** | âŒ æ—  | âœ… Qwen2VLProcessor |
| **å¤šæ¨¡æ€ç¼–ç ** | âŒ ä»…æ–‡æœ¬ | âœ… **æ–‡æœ¬+å›¾åƒ** |

### 5.2 ç¼–ç é˜¶æ®µå·®å¼‚

| é˜¶æ®µ | åŸç‰ˆ Qwen-Image | Edit ç‰ˆæœ¬ |
|-----|----------------|----------|
| **æ–‡æœ¬ç¼–ç ** | Tokenizer â†’ Text Encoder (çº¯æ–‡æœ¬) | Processor â†’ Text Encoder (**å¤šæ¨¡æ€**) |
| **å›¾åƒç¼–ç è·¯å¾„1** | âŒ æ—  | âœ… **Qwen2.5-VL** (è¯­ä¹‰ç†è§£) |
| **å›¾åƒç¼–ç è·¯å¾„2** | âŒ æ—  | âœ… **VAE Encoder** (è§†è§‰å¤–è§‚) |
| **Prompt Embeddings** | ä»…æ–‡æœ¬è¯­ä¹‰ | æ–‡æœ¬è¯­ä¹‰ + å›¾åƒè¯­ä¹‰ |

### 5.3 å»å™ªå¾ªç¯å·®å¼‚

| ç‰¹æ€§ | åŸç‰ˆ Qwen-Image | Edit ç‰ˆæœ¬ |
|-----|----------------|----------|
| **åˆå§‹ Latents** | éšæœºå™ªå£° | éšæœºå™ªå£° + **å›¾åƒ latents** |
| **Transformer è¾“å…¥** | ä»…å»å™ª latents | **æ‹¼æ¥** [å»å™ª latents, å›¾åƒ latents] |
| **æ¡ä»¶æ§åˆ¶** | æ–‡æœ¬ embeddings | **æ–‡æœ¬+å›¾åƒ** embeddings |

---

## ğŸ’¡ å…­ã€æŠ€æœ¯ä¼˜åŠ¿åˆ†æ

### 6.1 åŒè·¯å¾„æ¶æ„çš„ä¼˜åŠ¿

#### è¯­ä¹‰ç†è§£è·¯å¾„ï¼ˆQwen2.5-VLï¼‰
- âœ… **ç†è§£å›¾åƒå†…å®¹**ï¼šè¯†åˆ«å¯¹è±¡ã€åœºæ™¯ã€å…³ç³»
- âœ… **ç†è§£ç¼–è¾‘æ„å›¾**ï¼šå°†æ–‡æœ¬æŒ‡ä»¤æ˜ å°„åˆ°è§†è§‰ä¿®æ”¹
- âœ… **ä¿æŒè¯­ä¹‰ä¸€è‡´æ€§**ï¼šåœ¨ç¼–è¾‘æ—¶ä¿ç•™åŸå›¾çš„è¯­ä¹‰ç»“æ„

#### è§†è§‰å¤–è§‚è·¯å¾„ï¼ˆVAEï¼‰
- âœ… **ä¿æŒè§†è§‰ç»†èŠ‚**ï¼šä¿ç•™çº¹ç†ã€é¢œè‰²ã€å…‰ç…§
- âœ… **ç²¾ç¡®åŒºåŸŸæ§åˆ¶**ï¼šåœ¨éœ€è¦ä¿æŒä¸å˜çš„åŒºåŸŸæä¾›å‚è€ƒ
- âœ… **å¤–è§‚ä¸€è‡´æ€§**ï¼šç¡®ä¿ç¼–è¾‘åçš„å›¾åƒä¸åŸå›¾è§†è§‰é£æ ¼ä¸€è‡´

### 6.2 Edit ç‰ˆæœ¬çš„åˆ›æ–°ç‚¹

1. **åŒè·¯å¾„å¹¶è¡Œå¤„ç†**
   - è¯­ä¹‰è·¯å¾„ï¼šç†è§£"è¦åšä»€ä¹ˆ"
   - å¤–è§‚è·¯å¾„ï¼šå‚è€ƒ"æ€ä¹ˆåš"

2. **Latent æ‹¼æ¥ç­–ç•¥**
   - åœ¨å»å™ªè¿‡ç¨‹ä¸­æŒç»­æ³¨å…¥åŸå§‹å›¾åƒä¿¡æ¯
   - å…è®¸ Transformer åŒæ—¶è®¿é—®ç¼–è¾‘çŠ¶æ€å’Œå‚è€ƒçŠ¶æ€

3. **å¤šæ¨¡æ€æ¡ä»¶èåˆ**
   - æ–‡æœ¬æŒ‡ä»¤ä¸å›¾åƒç†è§£åœ¨åŒä¸€ä¸ª embedding ç©ºé—´ä¸­èåˆ
   - å®ç°äº†è¯­ä¹‰å’Œè§†è§‰çš„ç»Ÿä¸€æ§åˆ¶

---

## ğŸ“Š ä¸ƒã€ä»£ç è¡Œæ•°å¯¹æ¯”

| æ–‡ä»¶ | åŸç‰ˆ Qwen-Image | Edit ç‰ˆæœ¬ | å·®å¼‚ |
|------|----------------|-----------|------|
| **æ€»è¡Œæ•°** | ~772 è¡Œ | ~900 è¡Œ | +128 è¡Œ |
| **åˆå§‹åŒ–** | ~40 è¡Œ | ~45 è¡Œ | +5 è¡Œ (processor) |
| **æ–‡æœ¬ç¼–ç ** | ~38 è¡Œ | ~46 è¡Œ | +8 è¡Œ (å¤šæ¨¡æ€) |
| **Latent å‡†å¤‡** | ~22 è¡Œ | ~54 è¡Œ | +32 è¡Œ (VAEç¼–ç ) |
| **å»å™ªå¾ªç¯** | ~70 è¡Œ | ~75 è¡Œ | +5 è¡Œ (latentæ‹¼æ¥) |

---

## ğŸ”¬ å…«ã€è¯¦ç»†ä»£ç å¯¹ç…§è¡¨

### 8.1 å…³é”®æ–¹æ³•å¯¹æ¯”

| æ–¹æ³• | åŸç‰ˆ | Edit ç‰ˆæœ¬ | å·®å¼‚è¯´æ˜ |
|------|------|----------|---------|
| `__init__` | æ—  processor | âœ… æœ‰ processor | æ”¯æŒå¤šæ¨¡æ€è¾“å…¥ |
| `_get_qwen_prompt_embeds` | `(prompt, device)` | `(prompt, **image**, device)` | â­ æ–°å¢å›¾åƒå‚æ•° |
| `encode_prompt` | `(prompt, device, ...)` | `(prompt, **image**, device, ...)` | â­ æ–°å¢å›¾åƒå‚æ•° |
| `prepare_latents` | ä»…ç”Ÿæˆéšæœºå™ªå£° | âœ… **ç¼–ç å›¾åƒ + ç”Ÿæˆå™ªå£°** | â­ æ–°å¢ VAE ç¼–ç  |
| `_encode_vae_image` | âŒ æ—  | âœ… æ–°å¢æ–¹æ³• | â­ Edit ç‰ˆæœ¬ç‰¹æœ‰ |
| å»å™ªå¾ªç¯ä¸­çš„ `latent_model_input` | `latents` | `cat([latents, image_latents])` | â­ æ‹¼æ¥æ“ä½œ |

### 8.2 Prompt æ¨¡æ¿å¯¹æ¯”

| ç‰¹æ€§ | åŸç‰ˆ | Edit ç‰ˆæœ¬ |
|------|------|----------|
| **ç³»ç»Ÿæç¤ºé•¿åº¦** | 34 tokens | 64 tokens |
| **æ˜¯å¦åŒ…å«å›¾åƒå ä½ç¬¦** | âŒ | âœ… `<|vision_start|><|image_pad|><|vision_end|>` |
| **æ ¸å¿ƒä»»åŠ¡** | æè¿°å›¾åƒ | **ç†è§£ç¼–è¾‘æŒ‡ä»¤å¹¶ç”Ÿæˆæ–°å›¾åƒ** |

---

## ğŸ“ ä¹ã€è®¾è®¡ç†å¿µå¯¹æ¯”

### 9.1 åŸç‰ˆ Qwen-Image è®¾è®¡ç†å¿µ

- **ç›®æ ‡**ï¼šä»æ–‡æœ¬ç”Ÿæˆå…¨æ–°å›¾åƒ
- **ç­–ç•¥**ï¼šçº¯æ–‡æœ¬æ¡ä»¶ç”Ÿæˆ
- **ä¼˜åŠ¿**ï¼šç”Ÿæˆé€Ÿåº¦å¿«ï¼Œé€‚åˆåˆ›æ„åˆ›ä½œ

### 9.2 Edit ç‰ˆæœ¬è®¾è®¡ç†å¿µ

- **ç›®æ ‡**ï¼šåœ¨ç°æœ‰å›¾åƒåŸºç¡€ä¸Šç²¾ç¡®ç¼–è¾‘
- **ç­–ç•¥**ï¼š**åŒè·¯å¾„æ¡ä»¶æ§åˆ¶**ï¼ˆè¯­ä¹‰+å¤–è§‚ï¼‰
- **ä¼˜åŠ¿**ï¼š
  1. âœ… ä¿æŒåŸå›¾ä¸€è‡´æ€§
  2. âœ… ç†è§£ç¼–è¾‘ä¸Šä¸‹æ–‡
  3. âœ… ç²¾ç¡®æ§åˆ¶ç¼–è¾‘åŒºåŸŸ
  4. âœ… æ”¯æŒè¯­ä¹‰ç¼–è¾‘å’Œå¤–è§‚ç¼–è¾‘

---

## ğŸ“ åã€ä½¿ç”¨åœºæ™¯å¯¹æ¯”

### åŸç‰ˆ Qwen-Image é€‚ç”¨åœºæ™¯
- ğŸ¨ ä»é›¶å¼€å§‹ç”Ÿæˆå›¾åƒ
- ğŸ–¼ï¸ åˆ›æ„è®¾è®¡
- ğŸ“ æ–‡æœ¬åˆ°å›¾åƒè½¬æ¢

### Edit ç‰ˆæœ¬é€‚ç”¨åœºæ™¯
- âœï¸ å›¾åƒç¼–è¾‘ï¼ˆé¢œè‰²ã€å¯¹è±¡ã€èƒŒæ™¯ç­‰ï¼‰
- ğŸ”„ é£æ ¼è½¬æ¢
- ğŸ“‹ æ–‡æœ¬ç²¾ç¡®ç¼–è¾‘
- ğŸ¯ å±€éƒ¨ä¿®æ”¹ï¼ˆæ·»åŠ /åˆ é™¤/æ›¿æ¢å¯¹è±¡ï¼‰
- ğŸŒˆ è¯­ä¹‰ç¼–è¾‘ï¼ˆè§†è§’å˜åŒ–ã€é£æ ¼è½¬æ¢ï¼‰

---

## ğŸ”š åä¸€ã€æ€»ç»“

Qwen-Image-Edit çš„æ ¸å¿ƒåˆ›æ–°åœ¨äº**åŒè·¯å¾„æ¶æ„**ï¼š

1. **è¯­ä¹‰è·¯å¾„ï¼ˆQwen2.5-VLï¼‰**ï¼šå›¾åƒ + æ–‡æœ¬ â†’ å¤šæ¨¡æ€ç†è§£ â†’ ç¼–è¾‘æŒ‡ä»¤ç†è§£
2. **å¤–è§‚è·¯å¾„ï¼ˆVAEï¼‰**ï¼šå›¾åƒ â†’ è§†è§‰ç‰¹å¾ç¼–ç  â†’ å¤–è§‚å‚è€ƒ

é€šè¿‡è¿™ä¸¤æ¡è·¯å¾„çš„ååŒå·¥ä½œï¼ŒEdit ç‰ˆæœ¬èƒ½å¤Ÿåœ¨ä¿æŒåŸå›¾ä¸€è‡´æ€§çš„åŒæ—¶ï¼Œå®ç°ç²¾ç¡®çš„è¯­ä¹‰å’Œå¤–è§‚ç¼–è¾‘ã€‚

**å…³é”®ä»£ç å·®å¼‚ä½ç½®**ï¼š
- æ–‡æœ¬ç¼–ç ï¼š```226:271:pipelines/qwenimage/pipeline_qwenimage_edit.py```
- VAE ç¼–ç ï¼š```395:416:pipelines/qwenimage/pipeline_qwenimage_edit.py```
- å»å™ªå¾ªç¯ï¼š```810:828:pipelines/qwenimage/pipeline_qwenimage_edit.py```

---

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**ï¼šåŸºäº diffusers ä»£ç åº“åˆ†æ  
**ç‰ˆæœ¬**ï¼šdiffusers 0.36.0.dev0


