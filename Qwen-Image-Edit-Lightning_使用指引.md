# Qwen-Image-Edit-2509-Lightning æ¨¡å‹ä½¿ç”¨æŒ‡å¼•

## ğŸ“‹ æ¨¡å‹ä¿¡æ¯

- **æ¨¡å‹åç§°**: Qwen-Image-Edit-2509-Lightning-4steps-V1.0-bf16
- **æ¨¡å‹è·¯å¾„**: `lightx2v/Qwen-Image-Lightning/Qwen-Image-Edit-2509/`
- **æ¨¡å‹æ–‡ä»¶**: `Qwen-Image-Edit-2509-Lightning-4steps-V1.0-bf16.safetensors`
- **ç‰¹ç‚¹**: çŸ¥è¯†è’¸é¦æ¨¡å‹ï¼Œä»…éœ€ **4 æ­¥æ¨ç†**ï¼ˆåŸç‰ˆéœ€ 50 æ­¥ï¼‰
- **ç²¾åº¦**: bfloat16

---

## ğŸ”½ ä¸€ã€æ¨¡å‹ä¸‹è½½

### æ–¹æ³•1ï¼šä½¿ç”¨ Hugging Face CLI ä¸‹è½½

```bash
# å®‰è£… huggingface_hub
pip install huggingface_hub

# ä¸‹è½½æ¨¡å‹æ–‡ä»¶
huggingface-cli download lightx2v/Qwen-Image-Lightning \
    Qwen-Image-Edit-2509/Qwen-Image-Edit-2509-Lightning-4steps-V1.0-bf16.safetensors \
    --local-dir ./models/lightning \
    --local-dir-use-symlinks False
```

### æ–¹æ³•2ï¼šä½¿ç”¨ Python è„šæœ¬ä¸‹è½½

åˆ›å»º `download_lightning_model.py`ï¼š

```python
from huggingface_hub import hf_hub_download
import os

# é…ç½®è·¯å¾„
repo_id = "lightx2v/Qwen-Image-Lightning"
filename = "Qwen-Image-Edit-2509/Qwen-Image-Edit-2509-Lightning-4steps-V1.0-bf16.safetensors"
local_dir = "./models/lightning"

# åˆ›å»ºç›®å½•
os.makedirs(local_dir, exist_ok=True)

# ä¸‹è½½æ¨¡å‹
print(f"æ­£åœ¨ä¸‹è½½ {filename}...")
model_path = hf_hub_download(
    repo_id=repo_id,
    filename=filename,
    local_dir=local_dir,
    local_dir_use_symlinks=False
)

print(f"æ¨¡å‹å·²ä¸‹è½½åˆ°: {model_path}")
```

è¿è¡Œï¼š
```bash
python download_lightning_model.py
```

### æ–¹æ³•3ï¼šæ‰‹åŠ¨ä¸‹è½½

1. è®¿é—®ï¼šhttps://huggingface.co/lightx2v/Qwen-Image-Lightning/tree/main/Qwen-Image-Edit-2509
2. ä¸‹è½½ `Qwen-Image-Edit-2509-Lightning-4steps-V1.0-bf16.safetensors`
3. ä¿å­˜åˆ°ï¼š`./models/lightning/Qwen-Image-Edit-2509-Lightning-4steps-V1.0-bf16.safetensors`

---

## ğŸš€ äºŒã€ä½¿ç”¨æ–¹æ³•

### æ–¹æ³•1ï¼šç›´æ¥åŠ è½½ Lightning æ¨¡å‹ï¼ˆæ¨èï¼‰

Lightning æ¨¡å‹é€šå¸¸æ˜¯**å®Œæ•´çš„ transformer æƒé‡**ï¼Œå¯ä»¥ç›´æ¥æ›¿æ¢åŸæ¨¡å‹çš„ transformer éƒ¨åˆ†ã€‚

**æ­¥éª¤1ï¼šåˆ›å»ºåŠ è½½è„šæœ¬**

åˆ›å»º `load_lightning_model.py`ï¼š

```python
import torch
from diffusers import QwenImageEditPipeline
from safetensors.torch import load_file

# 1. åŠ è½½åŸå§‹ Qwen-Image-Edit Pipelineï¼ˆåŒ…å« VAE, Text Encoder ç­‰ï¼‰
print("åŠ è½½åŸºç¡€ Pipeline...")
pipeline = QwenImageEditPipeline.from_pretrained(
    "Qwen/Qwen-Image-Edit",
    torch_dtype=torch.bfloat16
)

# 2. åŠ è½½ Lightning Transformer æƒé‡
lightning_model_path = "./models/lightning/Qwen-Image-Edit-2509-Lightning-4steps-V1.0-bf16.safetensors"
print(f"åŠ è½½ Lightning æ¨¡å‹: {lightning_model_path}")

# åŠ è½½ safetensors æ–‡ä»¶
lightning_state_dict = load_file(lightning_model_path)

# 3. å°† Lightning æƒé‡åŠ è½½åˆ° Transformer
# æ³¨æ„ï¼šç¡®ä¿æƒé‡é”®ååŒ¹é…
pipeline.transformer.load_state_dict(lightning_state_dict, strict=False)

# 4. ç§»åŠ¨åˆ° GPU
pipeline.to("cuda")

print("Lightning æ¨¡å‹åŠ è½½å®Œæˆï¼")
print(f"Transformer è®¾å¤‡: {next(pipeline.transformer.parameters()).device}")
print(f"Transformer ç²¾åº¦: {next(pipeline.transformer.parameters()).dtype}")

# ä¿å­˜å®Œæ•´ Pipelineï¼ˆå¯é€‰ï¼‰
# pipeline.save_pretrained("./models/qwen-image-edit-lightning")
```

**æ­¥éª¤2ï¼šè¿è¡Œæ¨ç†ï¼ˆ4 æ­¥ï¼‰**

åˆ›å»º `inference_lightning.py`ï¼š

```python
import torch
from PIL import Image
from diffusers import QwenImageEditPipeline
from safetensors.torch import load_file

# åŠ è½½ Pipelineï¼ˆä½¿ç”¨ä¸Šé¢çš„æ–¹æ³•ï¼‰
pipeline = QwenImageEditPipeline.from_pretrained(
    "Qwen/Qwen-Image-Edit",
    torch_dtype=torch.bfloat16
)

# åŠ è½½ Lightning æƒé‡
lightning_state_dict = load_file(
    "./models/lightning/Qwen-Image-Edit-2509-Lightning-4steps-V1.0-bf16.safetensors"
)
pipeline.transformer.load_state_dict(lightning_state_dict, strict=False)
pipeline.to("cuda")

# å‡†å¤‡è¾“å…¥
image = Image.open("input.png").convert("RGB")
prompt = "Change the rabbit's color to purple, with a flash light background."

# â­ å…³é”®ï¼šLightning æ¨¡å‹åªéœ€ 4 æ­¥æ¨ç†
inputs = {
    "image": image,
    "prompt": prompt,
    "generator": torch.manual_seed(0),
    "true_cfg_scale": 4.0,
    "negative_prompt": " ",  # ç©ºå­—ç¬¦ä¸²ä¹Ÿå¯ä»¥
    "num_inference_steps": 4,  # â­ Lightning: 4 æ­¥ï¼ˆåŸç‰ˆ 50 æ­¥ï¼‰
}

# æ¨ç†
print("å¼€å§‹æ¨ç†ï¼ˆ4 æ­¥ï¼‰...")
with torch.inference_mode():
    output = pipeline(**inputs)
    output_image = output.images[0]
    output_image.save("output_lightning_4steps.png")
    print("æ¨ç†å®Œæˆï¼è¾“å‡ºä¿å­˜è‡³: output_lightning_4steps.png")
```

---

### æ–¹æ³•2ï¼šæ£€æŸ¥æƒé‡ç»“æ„å¹¶æ‰‹åŠ¨æ˜ å°„

å¦‚æœæƒé‡é”®åä¸åŒ¹é…ï¼Œéœ€è¦æ‰‹åŠ¨æ˜ å°„ï¼š

åˆ›å»º `check_and_load_lightning.py`ï¼š

```python
import torch
from safetensors.torch import load_file
from diffusers import QwenImageEditPipeline

# åŠ è½½åŸå§‹ pipeline
pipeline = QwenImageEditPipeline.from_pretrained(
    "Qwen/Qwen-Image-Edit",
    torch_dtype=torch.bfloat16
)

# åŠ è½½ Lightning æƒé‡
lightning_path = "./models/lightning/Qwen-Image-Edit-2509-Lightning-4steps-V1.0-bf16.safetensors"
lightning_state_dict = load_file(lightning_path)

# æ£€æŸ¥æƒé‡é”®å
print("=== Lightning æ¨¡å‹é”®åï¼ˆå‰10ä¸ªï¼‰===")
for i, key in enumerate(list(lightning_state_dict.keys())[:10]):
    print(f"{i+1}. {key}")

print("\n=== Transformer åŸå§‹é”®åï¼ˆå‰10ä¸ªï¼‰===")
transformer_state_dict = pipeline.transformer.state_dict()
for i, key in enumerate(list(transformer_state_dict.keys())[:10]):
    print(f"{i+1}. {key}")

# å°è¯•åŒ¹é…é”®å
print("\n=== å°è¯•åŠ è½½ ===")
try:
    pipeline.transformer.load_state_dict(lightning_state_dict, strict=False)
    print("âœ… åŠ è½½æˆåŠŸï¼ˆä½¿ç”¨ strict=Falseï¼‰")
except Exception as e:
    print(f"âŒ åŠ è½½å¤±è´¥: {e}")
    print("\néœ€è¦æ‰‹åŠ¨æ˜ å°„é”®å...")
```

---

## ğŸ”„ ä¸‰ã€LoRA åˆå¹¶ï¼ˆå¦‚æœé€‚ç”¨ï¼‰

### åˆ¤æ–­æ˜¯å¦ä¸º LoRA æƒé‡

æ£€æŸ¥æƒé‡é”®åæ˜¯å¦åŒ…å« `lora_A`ã€`lora_B` æˆ– `alpha`ï¼š

```python
from safetensors.torch import load_file

lightning_state_dict = load_file(
    "./models/lightning/Qwen-Image-Edit-2509-Lightning-4steps-V1.0-bf16.safetensors"
)

# æ£€æŸ¥æ˜¯å¦ä¸º LoRA
is_lora = any("lora_A" in key or "lora_B" in key for key in lightning_state_dict.keys())

if is_lora:
    print("è¿™æ˜¯ LoRA æƒé‡ï¼Œéœ€è¦åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹")
    # ä½¿ç”¨ pipeline çš„ load_lora_weights æ–¹æ³•
else:
    print("è¿™æ˜¯å®Œæ•´æ¨¡å‹æƒé‡ï¼Œå¯ä»¥ç›´æ¥åŠ è½½")
```

### å¦‚æœæ˜¯ LoRAï¼Œä½¿ç”¨ä»¥ä¸‹æ–¹æ³•åˆå¹¶

**æ–¹æ³•Aï¼šè¿è¡Œæ—¶åŠ è½½ï¼ˆä¸åˆå¹¶ï¼‰**

```python
from diffusers import QwenImageEditPipeline
import torch

pipeline = QwenImageEditPipeline.from_pretrained(
    "Qwen/Qwen-Image-Edit",
    torch_dtype=torch.bfloat16
)

# åŠ è½½ LoRAï¼ˆè¿è¡Œæ—¶ï¼‰
pipeline.load_lora_weights(
    "./models/lightning",
    weight_name="Qwen-Image-Edit-2509-Lightning-4steps-V1.0-bf16.safetensors"
)

# è®¾ç½® LoRA scaleï¼ˆå¯é€‰ï¼‰
# pipeline.fuse_lora(lora_scale=1.0)  # èåˆ LoRA åˆ°æƒé‡ä¸­

pipeline.to("cuda")
```

**æ–¹æ³•Bï¼šåˆå¹¶ LoRA åˆ°æƒé‡ï¼ˆæ°¸ä¹…ï¼‰**

```python
from diffusers import QwenImageEditPipeline
import torch

pipeline = QwenImageEditPipeline.from_pretrained(
    "Qwen/Qwen-Image-Edit",
    torch_dtype=torch.bfloat16
)

# åŠ è½½ LoRA
pipeline.load_lora_weights(
    "./models/lightning",
    weight_name="Qwen-Image-Edit-2509-Lightning-4steps-V1.0-bf16.safetensors"
)

# â­ èåˆ LoRA åˆ°åŸºç¡€æƒé‡ï¼ˆæ°¸ä¹…åˆå¹¶ï¼‰
pipeline.fuse_lora(lora_scale=1.0)

# ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
pipeline.save_pretrained("./models/qwen-image-edit-lightning-merged")

# ä¹‹åå¯ä»¥ç›´æ¥åŠ è½½åˆå¹¶åçš„æ¨¡å‹
# pipeline = QwenImageEditPipeline.from_pretrained(
#     "./models/qwen-image-edit-lightning-merged",
#     torch_dtype=torch.bfloat16
# )
```

---

## ğŸ”§ å››ã€æ›¿æ¢åŸæ¨¡å‹æ–‡ä»¶

### æ–¹æ¡ˆ1ï¼šæ›¿æ¢ Transformer æƒé‡æ–‡ä»¶

å¦‚æœ Lightning æ¨¡å‹æ˜¯å®Œæ•´çš„ transformer æƒé‡ï¼š

```bash
# 1. æ‰¾åˆ°åŸæ¨¡å‹çš„ transformer æƒé‡ä½ç½®
# é€šå¸¸åœ¨ï¼š~/.cache/huggingface/hub/models--Qwen--Qwen-Image-Edit/snapshots/.../transformer/

# 2. å¤‡ä»½åŸæ–‡ä»¶
cp transformer/model.safetensors transformer/model.safetensors.backup

# 3. æ›¿æ¢ä¸º Lightning æ¨¡å‹
cp ./models/lightning/Qwen-Image-Edit-2509-Lightning-4steps-V1.0-bf16.safetensors \
   transformer/model.safetensors

# 4. ä½¿ç”¨åŸ pipeline åŠ è½½ï¼ˆä¼šè‡ªåŠ¨ä½¿ç”¨æ–°æƒé‡ï¼‰
```

### æ–¹æ¡ˆ2ï¼šåˆ›å»ºæ–°çš„æ¨¡å‹ç›®å½•

```bash
# 1. å¤åˆ¶æ•´ä¸ªæ¨¡å‹ç›®å½•
cp -r ~/.cache/huggingface/hub/models--Qwen--Qwen-Image-Edit/snapshots/<latest> \
      ./models/qwen-image-edit-lightning

# 2. æ›¿æ¢ transformer æƒé‡
cp ./models/lightning/Qwen-Image-Edit-2509-Lightning-4steps-V1.0-bf16.safetensors \
   ./models/qwen-image-edit-lightning/transformer/model.safetensors

# 3. ä»æ–°ç›®å½•åŠ è½½
# pipeline = QwenImageEditPipeline.from_pretrained(
#     "./models/qwen-image-edit-lightning",
#     torch_dtype=torch.bfloat16
# )
```

---

## ğŸ“ äº”ã€å®Œæ•´ä½¿ç”¨ç¤ºä¾‹

åˆ›å»º `lightning_inference_complete.py`ï¼š

```python
"""
Qwen-Image-Edit Lightning æ¨¡å‹å®Œæ•´æ¨ç†ç¤ºä¾‹
"""
import torch
from PIL import Image
from diffusers import QwenImageEditPipeline
from safetensors.torch import load_file
import os

def load_lightning_pipeline(
    base_model="Qwen/Qwen-Image-Edit",
    lightning_model_path="./models/lightning/Qwen-Image-Edit-2509-Lightning-4steps-V1.0-bf16.safetensors",
    device="cuda",
    dtype=torch.bfloat16
):
    """
    åŠ è½½ Lightning æ¨¡å‹ Pipeline
    
    Args:
        base_model: åŸºç¡€æ¨¡å‹è·¯å¾„ï¼ˆHugging Face ID æˆ–æœ¬åœ°è·¯å¾„ï¼‰
        lightning_model_path: Lightning æ¨¡å‹æƒé‡è·¯å¾„
        device: è®¾å¤‡
        dtype: æ•°æ®ç±»å‹
    """
    print(f"1. åŠ è½½åŸºç¡€ Pipeline: {base_model}")
    pipeline = QwenImageEditPipeline.from_pretrained(
        base_model,
        torch_dtype=dtype
    )
    
    if os.path.exists(lightning_model_path):
        print(f"2. åŠ è½½ Lightning æ¨¡å‹: {lightning_model_path}")
        lightning_state_dict = load_file(lightning_model_path)
        
        # å°è¯•åŠ è½½æƒé‡
        try:
            pipeline.transformer.load_state_dict(lightning_state_dict, strict=True)
            print("   âœ… ç²¾ç¡®åŒ¹é…åŠ è½½æˆåŠŸ")
        except:
            print("   âš ï¸ ç²¾ç¡®åŒ¹é…å¤±è´¥ï¼Œå°è¯•å®½æ¾åŠ è½½...")
            missing, unexpected = pipeline.transformer.load_state_dict(
                lightning_state_dict, strict=False
            )
            if missing:
                print(f"   âš ï¸ ç¼ºå¤±é”®: {missing[:5]}...")
            if unexpected:
                print(f"   âš ï¸ é¢å¤–é”®: {unexpected[:5]}...")
    else:
        print(f"   âš ï¸ Lightning æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {lightning_model_path}")
        print("   ä½¿ç”¨åŸå§‹æ¨¡å‹ï¼ˆ50æ­¥æ¨ç†ï¼‰")
    
    pipeline.to(device)
    return pipeline

def run_inference(
    pipeline,
    image_path="input.png",
    prompt="Change the rabbit's color to purple, with a flash light background.",
    output_path="output_lightning.png",
    num_steps=4,  # Lightning: 4æ­¥
    true_cfg_scale=4.0,
    negative_prompt=" ",
):
    """
    è¿è¡Œæ¨ç†
    """
    # åŠ è½½å›¾åƒ
    image = Image.open(image_path).convert("RGB")
    
    # å‡†å¤‡è¾“å…¥
    inputs = {
        "image": image,
        "prompt": prompt,
        "generator": torch.manual_seed(0),
        "true_cfg_scale": true_cfg_scale,
        "negative_prompt": negative_prompt,
        "num_inference_steps": num_steps,  # â­ Lightning: 4æ­¥
    }
    
    # æ¨ç†
    print(f"\n3. å¼€å§‹æ¨ç†ï¼ˆ{num_steps}æ­¥ï¼‰...")
    with torch.inference_mode():
        output = pipeline(**inputs)
        output_image = output.images[0]
        output_image.save(output_path)
        print(f"   âœ… æ¨ç†å®Œæˆï¼è¾“å‡º: {output_path}")
    
    return output_image

if __name__ == "__main__":
    # åŠ è½½ Pipeline
    pipeline = load_lightning_pipeline()
    
    # è¿è¡Œæ¨ç†
    run_inference(
        pipeline,
        image_path="input.png",
        prompt="Change the rabbit's color to purple, with a flash light background.",
        output_path="output_lightning_4steps.png",
        num_steps=4,  # Lightning æ¨¡å‹
    )
    
    print("\nâœ… å®Œæˆï¼")
```

---

## âš™ï¸ å…­ã€å…³é”®å‚æ•°è¯´æ˜

### Lightning æ¨¡å‹ä¸“ç”¨å‚æ•°

| å‚æ•° | åŸç‰ˆå€¼ | Lightning å€¼ | è¯´æ˜ |
|------|--------|--------------|------|
| `num_inference_steps` | 50 | **4** | â­ æ¨ç†æ­¥æ•° |
| `true_cfg_scale` | 4.0 | 4.0 | CFG å¼ºåº¦ï¼ˆå¯ä¿æŒï¼‰ |
| `torch_dtype` | bfloat16 | bfloat16 | ç²¾åº¦ï¼ˆåŒ¹é…æ¨¡å‹ï¼‰ |

### æ³¨æ„äº‹é¡¹

1. **æ¨ç†æ­¥æ•°**ï¼šLightning æ¨¡å‹è®¾è®¡ä¸º 4 æ­¥ï¼Œä½¿ç”¨æ›´å¤šæ­¥æ•°å¯èƒ½ä¸ä¼šæå‡è´¨é‡
2. **æ¨¡å‹å…¼å®¹æ€§**ï¼šç¡®ä¿ Lightning æ¨¡å‹ä¸åŸºç¡€æ¨¡å‹ç‰ˆæœ¬åŒ¹é…ï¼ˆ2509 ç‰ˆæœ¬ï¼‰
3. **æƒé‡æ ¼å¼**ï¼šå¦‚æœæ˜¯ safetensors æ ¼å¼ï¼Œéœ€è¦ä½¿ç”¨ `load_file` åŠ è½½
4. **è®¾å¤‡å†…å­˜**ï¼šLightning æ¨¡å‹æ¨ç†æ›´å¿«ï¼Œä½†æ¨¡å‹å¤§å°å¯èƒ½ç›¸åŒ

---

## ğŸ› ä¸ƒã€å¸¸è§é—®é¢˜æ’æŸ¥

### é—®é¢˜1ï¼šæƒé‡é”®åä¸åŒ¹é…

**ç—‡çŠ¶**ï¼š`load_state_dict` æŠ¥é”™ï¼Œæç¤ºé”®åä¸åŒ¹é…

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# ä½¿ç”¨ strict=False
pipeline.transformer.load_state_dict(lightning_state_dict, strict=False)

# æˆ–æ‰‹åŠ¨æ˜ å°„é”®å
def map_keys(old_dict, key_mapping):
    new_dict = {}
    for old_key, new_key in key_mapping.items():
        if old_key in old_dict:
            new_dict[new_key] = old_dict[old_key]
    return new_dict
```

### é—®é¢˜2ï¼šæ¨¡å‹ç»“æ„ä¸åŒ¹é…

**ç—‡çŠ¶**ï¼šæƒé‡å½¢çŠ¶ä¸åŒ¹é…

**è§£å†³æ–¹æ¡ˆ**ï¼š
- ç¡®è®¤ Lightning æ¨¡å‹æ˜¯å¦ä¸åŸºç¡€æ¨¡å‹ç‰ˆæœ¬åŒ¹é…
- æ£€æŸ¥æ¨¡å‹é…ç½®æ–‡ä»¶ï¼ˆconfig.jsonï¼‰

### é—®é¢˜3ï¼šæ¨ç†ç»“æœä¸ç†æƒ³

**è§£å†³æ–¹æ¡ˆ**ï¼š
- ç¡®è®¤ä½¿ç”¨ `num_inference_steps=4`
- æ£€æŸ¥ `true_cfg_scale` å‚æ•°
- å°è¯•ä¸åŒçš„ `negative_prompt`

---

## ğŸ“š å…«ã€å‚è€ƒèµ„æº

1. **æ¨¡å‹ä»“åº“**: https://huggingface.co/lightx2v/Qwen-Image-Lightning
2. **åŸç‰ˆæ¨¡å‹**: https://huggingface.co/Qwen/Qwen-Image-Edit
3. **Diffusers æ–‡æ¡£**: https://huggingface.co/docs/diffusers
4. **Lightning è®ºæ–‡**: Knowledge Distillation for Fast Diffusion Models

---

## âœ… æ€»ç»“

1. **ä¸‹è½½æ¨¡å‹**ï¼šä½¿ç”¨ Hugging Face CLI æˆ– Python è„šæœ¬
2. **åŠ è½½æ–¹å¼**ï¼š
   - å¦‚æœæ˜¯å®Œæ•´æƒé‡ï¼šç›´æ¥ `load_state_dict`
   - å¦‚æœæ˜¯ LoRAï¼šä½¿ç”¨ `load_lora_weights` + `fuse_lora`
3. **æ¨ç†å‚æ•°**ï¼š`num_inference_steps=4`ï¼ˆå…³é”®ï¼‰
4. **æ›¿æ¢æ–‡ä»¶**ï¼šå¯ä»¥æ›¿æ¢åŸæ¨¡å‹çš„ transformer æƒé‡æ–‡ä»¶

**å…³é”®ä»£ç **ï¼š
```python
# åŠ è½½ Lightning æƒé‡
lightning_state_dict = load_file("lightning_model.safetensors")
pipeline.transformer.load_state_dict(lightning_state_dict, strict=False)

# 4æ­¥æ¨ç†
output = pipeline(image, prompt, num_inference_steps=4)
```

